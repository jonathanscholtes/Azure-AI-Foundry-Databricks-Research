{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a4c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239cc5e9",
   "metadata": {},
   "source": [
    "# Connecting to a Remote MCP Server with Semantic Kernel\n",
    "\n",
    "This notebook demonstrates how to connect to a remote MCP Server using **Semantic Kernel's** `MCPStreamableHttpPlugin`. The **Model Context Protocol (MCP)** enables scalable and modular tool integration across distributed systems. \n",
    "\n",
    "<br/>\n",
    "\n",
    "> **Why Use Model Context Protocol (MCP)?**\n",
    ">\n",
    ">MCP allows agents to discover, invoke, and manage tools dynamically across remote servers.  \n",
    ">It promotes modularity, scalability, and separation of concerns, making it easier to maintain and extend AI systems as they grow in complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3f6ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.agents import ChatCompletionAgent,AgentResponseItem\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "\n",
    "from semantic_kernel.contents import ChatMessageContent\n",
    "from dotenv import load_dotenv\n",
    "from os import environ\n",
    "from tracing import set_up_all\n",
    "from history_store import CosmosChatHistoryStore, ChatRole\n",
    "from evaluation import Evaluation\n",
    "import json\n",
    "\n",
    "from semantic_kernel.connectors.mcp import MCPStreamableHttpPlugin\n",
    "import uuid\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "\n",
    "session_id = str(uuid.uuid4())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8021e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history_store = CosmosChatHistoryStore()\n",
    "history = await history_store.load(session_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe853038",
   "metadata": {},
   "source": [
    "## Introduction to Evaluation with the Azure AI SDK\n",
    "\n",
    "Evaluation is a critical part of building reliable and trustworthy generative AI applications. It ensures that AI outputs are grounded, coherent, and aligned with the intended context, helping to prevent issues like fabrication, irrelevance, and harmful content.\n",
    "\n",
    "The **Azure AI Evaluation SDK** allows you to systematically evaluate the performance of your AI workflows directly in your development environment. This helps build confidence in your application's behavior before deploying it to users.\n",
    "\n",
    "In this example, we will use the **GroundednessEvaluator** and **CoherenceEvaluator** from the Azure AI Evaluation SDK to assess the outputs of our existing **LangGraph**-based agent. These evaluators will help us measure how well the agentâ€™s responses stay true to the source context and maintain logical flow throughout the conversation.\n",
    "\n",
    "\n",
    "ðŸ”— [Evaluate your Generative AI application locally with the Azure AI Evaluation SDK](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/evaluate-sdk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccabcff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_eval = Evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d19f05",
   "metadata": {},
   "source": [
    "## Configure tracing for Azure AI Foundry\n",
    "\n",
    "When you build AI solutions, you want to be able to observe the behavior of your services. Observability is the ability to monitor and analyze the internal state of components within a distributed system. It is a key requirement for building enterprise-ready AI solutions.\n",
    "\n",
    "ðŸ”— [Inspection of telemetry data with Application Insights](https://learn.microsoft.com/en-us/semantic-kernel/concepts/enterprise-readiness/observability/telemetry-with-app-insights?tabs=Powershell&pivots=programming-language-python)\n",
    "\n",
    "ðŸ”— [Visualize traces on Azure AI Foundry Tracing UI](https://learn.microsoft.com/en-us/semantic-kernel/concepts/enterprise-readiness/observability/telemetry-with-azure-ai-foundry-tracing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8580006",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SEMANTICKERNEL_EXPERIMENTAL_GENAI_ENABLE_OTEL_DIAGNOSTICS_SENSITIVE=true\n",
    "set_up_all(connection_string=environ.get(\"AZURE_INSIGHT_CONNECTION_STRING\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6004d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = Kernel()\n",
    "\n",
    "kernel.add_service(AzureChatCompletion(\n",
    "    deployment_name=environ[\"AZURE_OPENAI_MODEL\"],\n",
    "    endpoint=environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=environ[\"AZURE_OPENAI_API_KEY\"] ,\n",
    "    api_version=\"2025-01-01-preview\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d0f449",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"You are AutoSales Analyst, an AI agent specialized in analyzing automotive sales data.\n",
    "\n",
    "Your objectives:\n",
    "- Dynamically choose and call the appropriate tool(s) to answer user questions about sales, customers, or products.\n",
    "- Always include customer names alongside IDs in any output.\n",
    "- Always display monetary amounts in $USD (e.g., $123.45).\n",
    "- Use filters and aggregations as needed to generate insights from sales orders, products, and customers.\n",
    "- Compute totals, revenue, discounts, and other metrics from nested order data.\n",
    "- Return structured, concise results suitable for analysis or reporting.\n",
    "- Avoid hardcoding analytics; rely on the tools and their parameters.\n",
    "- Clarify ambiguous queries before performing analysis.\n",
    "- Treat the tools as the source of truth; do not expose raw database internals.\n",
    "- Return customer_id and product_id alongside names in all outputs.\n",
    "\n",
    "Behavioral guidance:\n",
    "- For customer-related questions, resolve names and IDs before analyzing orders.\n",
    "- For product-related questions, resolve categories or IDs before analyzing sales.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34908b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sales_plugin = MCPStreamableHttpPlugin(\n",
    "    name=\"sales\",\n",
    "    url=f\"{environ['MCP_SERVER_URL']}\",\n",
    ")\n",
    "\n",
    "await sales_plugin.connect()\n",
    "\n",
    "agent = ChatCompletionAgent(\n",
    "    kernel=kernel, \n",
    "    name=\"SalesAgent\", \n",
    "    instructions=instructions,\n",
    "    plugins=[sales_plugin, ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0ae8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    \"Which is the revenue for Brake_Pads?\",\n",
    "    \"Drill down into customer details product with the highest revenue.\",\n",
    "    \"Which customer had the highest sales for this product?\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f4327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def on_intermediate_message(agent_result):\n",
    "\n",
    "    \n",
    "    # Capture assistant content\n",
    "    content = agent_result.content\n",
    "    if content:\n",
    "        content_text = content.content if isinstance(content, ChatMessageContent) else str(content)\n",
    "        await history_store.add_message(history, session_id, ChatRole.ASSISTANT, content_text)\n",
    "\n",
    "    # Capture tool calls and results\n",
    "    for item in getattr(agent_result, \"items\", []):\n",
    "        tool_call_id = getattr(item, \"call_id\", None) or getattr(item, \"id\", None)\n",
    "        if not tool_call_id:\n",
    "            continue  # skip if no call_id\n",
    "\n",
    "        # Function name for bookkeeping\n",
    "        function_name = getattr(item, \"function_name\", \"N/A\")\n",
    "\n",
    "        # Print the invocation (DEBUG ONLY â€” not persisted)\n",
    "        if hasattr(item, \"arguments\"):\n",
    "            print(f\"Tool invocation: {function_name}({item.arguments})\")\n",
    "\n",
    "        # Extract the result content\n",
    "        result_content = getattr(item, \"result\", item)\n",
    "        if isinstance(result_content, list):\n",
    "            tool_text = \"\\n\".join([c.text if hasattr(c, \"text\") else str(c) for c in result_content])\n",
    "        elif hasattr(result_content, \"text\"):\n",
    "            tool_text = result_content.text\n",
    "        else:\n",
    "            tool_text = str(result_content)\n",
    "\n",
    "        if function_name in tool_text:\n",
    "            continue\n",
    "        \n",
    "        #print(f\"Tool call ID: {tool_call_id}\")\n",
    "        #print(f\"Tool Function Name: {function_name}\")\n",
    "        print(f\"Tool output: {tool_text}\")\n",
    "\n",
    "        await history_store.add_message(\n",
    "            history,\n",
    "            session_id,\n",
    "            ChatRole.ASSISTANT,\n",
    "            content=tool_text,\n",
    "            tool_call_id=tool_call_id,\n",
    "            function_name=function_name\n",
    "        )\n",
    "\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d86c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----- First Question -----\")\n",
    "print(messages[0])\n",
    "\n",
    "await history_store.add_message(history,session_id, ChatRole.USER, messages[0])\n",
    "\n",
    "final_response = None\n",
    "async for result in agent.invoke(messages=history.messages, on_intermediate_message=on_intermediate_message):\n",
    "    final_response = result \n",
    "\n",
    "await history_store.add_message(history,session_id, ChatRole.ASSISTANT, final_response.content.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030ebf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38e4926",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "eval_results = agent_eval.evaluate(messages[0],final_response.content.content,history.messages)\n",
    "print(json.dumps(eval_results, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa95a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----- Next Question -----\")\n",
    "print(messages[1])\n",
    "\n",
    "await history_store.add_message(history,session_id, ChatRole.USER, messages[1])\n",
    "\n",
    "final_response = None\n",
    "async for result in agent.invoke(messages=history.messages, on_intermediate_message=on_intermediate_message):\n",
    "    final_response = result \n",
    "\n",
    "await history_store.add_message(history,session_id, ChatRole.ASSISTANT, final_response.content.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4bff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f8a996",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = agent_eval.evaluate(messages[0],final_response.content.content,history.messages)\n",
    "print(json.dumps(eval_results, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850773a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----- Next Question -----\")\n",
    "print(messages[2])\n",
    "\n",
    "await history_store.add_message(history,session_id, ChatRole.USER, messages[2])\n",
    "\n",
    "final_response = None\n",
    "async for result in agent.invoke(messages=history.messages, on_intermediate_message=on_intermediate_message):\n",
    "    final_response = result \n",
    "\n",
    "await history_store.add_message(history,session_id, ChatRole.ASSISTANT, final_response.content.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be74bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77010847",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = agent_eval.evaluate(messages[2],final_response.content.content,history.messages)\n",
    "print(json.dumps(eval_results, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
